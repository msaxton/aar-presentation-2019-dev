---
layout: page
title: GPT-2
---
# GPT-2

GPT-2 is a provocative example of recent developments in AI. GPT-2 is a language model that was released in February 2019 by OpenAI. This model can be used to generate text to any given prompt. according to its creators,
> GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text.

 For a popular level overview of GPT-2's specifications and examples we recommend the [blog post](https://openai.com/blog/better-language-models/) from OpenAI which accompanied the initial release of the model as well as their [follow-up blog](https://openai.com/blog/gpt-2-6-month-follow-up/) from six months after the initial release. For those interested in a technical deep dive, OpenAI has also made available an academic [paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) discussing the technical details of the project.

Although GPT-2 is not the first language model able to generate text, what makes GPT-2 unique is that it was trained on a dataset of 8 million web pages. This large dataset allows GPT-2 to generate text which is often indistinguishable from human generated text. The text generation is not perfect, given it's initial data set it performs better with popular culture topics than with technical topics, but OpenAI reports one recent study which suggests that test subjects were often unable to distinguish text generated by GPT-2 from New York Times articles.

We encourage you to look at some examples of GPT-2 which are provided in the initial release [blog post](https://openai.com/blog/better-language-models/) or to generate some of your own examples using THIS RESOURCE.

GPT-2 is not only able to generate coherent texts, it is also able to answer questions. The following image is taken from OpenAI's paper, ["Language Models are Unsupervised Multitask Learners"](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

![Image of GPT-2 Q&A](https://iliff.github.io/aar-presentation-2019//assets/gpt2-questions.png)
