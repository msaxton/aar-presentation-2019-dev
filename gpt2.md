---
layout: page
title: GPT-2
---
# GPT-2

### What is GPT-2?

GPT-2 is a provocative example of recent developments in language-based AI. Released in February 2019 by the not-for-profit [OpenAI](https://openai) this model can generate text from any given prompt, even a single sentence. According to its creators, GPT-2 is trained with a simple objective: given the string of words in a text, predict the next word.  But GPT-2's predictive power goes far beyond the text prediction we have on our phones.  GPT-2 demonstrates that if a model can predict the next word, it can predict the one after that, and the one after that, etc., until it produces an essay of which it is the author.  OpenAI reports one recent study in which most test subjects were unable to distinguish between text generated by GPT-2 from articles produced by the New York Times.


### Why have we included GPT-2?

We have included GPT-2 in our project because it enables members of the seminar to have a direct encounter with an AI that may cause the feeling of uncanniness we think may be at the root of the human fear of AI.  A machine (i.e. computer) may be driving the production of text but watching it write its own unique text causes the eery feeling that some thing -- a mind? -- is "inside" it. Does this "something" resemble the human mind?


### More about GPT-2

To experience GPT-2, visit [TalkToTransformer](https://talktotransformer.com) and type any sentence--one you've written yourself or something from your favorite author. (TalktoTransformer comes courtesy of software engineer Adam King)  We hope that our individual and collective experience of GPT-2 will provide opportunity to discuss our reactions to it and investigate whether our reactions confirm or disconfirm our uncanniness hypothesis.

For an accessible overview of GPT-2's specifications and some impressive examples, we recommend the [blog post](https://openai.com/blog/better-language-models/) from OpenAI which accompanied the initial release of the model as well as their [follow-up blog](https://openai.com/blog/gpt-2-6-month-follow-up/) from six months after the initial release. For those interested in a technical deep dive, OpenAI has also made available an academic [paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) discussing the technical details of the project.

GPT-2 is not only able to generate coherent texts, it can answer questions. The following image is taken from OpenAI's paper, ["Language Models are Unsupervised Multitask Learners"](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

![Image of GPT-2 Q&A](https://iliff.github.io/aar-presentation-2019//assets/gpt2-questions.png)
